\documentclass[12pt,letter]{article}
\usepackage{../downey_format}



\begin{document}
	
	% set the section number, along with figure and equation numbers
	\setcounter{section}{2}	
	\setcounter{figure}{0}    
	\renewcommand\thefigure{\thesection.\arabic{figure}}


	\section{Transfer Functions}

		Thus-far, this text has only considered forced vibrations for 1-DOF systems excited with forcing functions that can be easily expressed using either sin or cos examples. Therefore, the previously developed solutions are only acceptable for systems with known and simple excitations. This chapter will introduce the concept of transfer functions for solving vibration related problems. The transfer function, in particular the Laplace transfer function, is an important tool in the study of vibrations as it allows the practitioner to solve for the temporal response of a system for a variety of inputs using a single approach. Examples of force excitation that can be calculated include using this method include:
		\begin{itemize}
			\item sinusoidal
			\item base excitation
			\item impulse
			\item arbitrary input
			\item arbitrary periodic input
		\end{itemize}

			Consider the following system

			\begin{figure}[H]
				\centering
				\includegraphics[]{../figures/control_system.png}
				\caption{Generic system $H$ subjected to an input $F$ and its corresponding output $X$.}
				\label{fig:control_system}
			\end{figure}

			\noindent where $F$ is the input, $H$ is the system, and $X$ is the output from the system. This formulation is called the transfer-function approach and is commonly used for the formulation and solution of dynamic problems in the control literature. It can also be used for solving the various forced-vibration problems including those from complex or stochastic inputs. 

\subsection{Laplace Transforms}




\begin{review}
	\label{sec:Laplace_review}
				
		Laplace transforms, or more boradly integral transform, are a procedure for integrating the time ($t$) dependence of a function into a function of of position or space ($s$). By transforming the whole whole differential equation from the time domain into a lower order function of space the problem becomes easier to solve as the function can often be manipulated algebraically. 

		
		\begin{figure}[H]
			\centering
			\includegraphics[width=2.71in]{../figures/Pierre-Simon_de_Laplace.jpg}
			\caption{Portrait of Pierre-Simon Laplace by Johann Ernst Heinsius (1775).\protect\footnotemark[1] }
			\label{fig:Pierre-Simon_de_Laplace}
		\end{figure}
		
		\footnotetext[1]{Johann Ernst Heinsius, CC BY-SA 4.0 $<$https://creativecommons.org/licenses/by-sa/4.0$>$, via Wikimedia Commons}  

		The Laplace transform is named after mathematician and astronomer Pierre-Simon Laplace (23 March 1749 - 5 March 1827 ). Pierre-Simon Laplace was one of the greatest scientists of all time and is often considered the French Newton. He taught Napoleon at the \'Ecole Militaire in 1784, became a count of the empire in 1806, and a marquis in 1817 after the restoration of the monarchy. He is credited with advancements in engineering, mathematics, statistics, physics, astronomy, and philosophy; however, maybe his greatest achievement is not only surviving but benefiting from the change from the Ancien R\'egime $\rightarrow$ Bonaparte $\rightarrow$ Bourbon Restoration. 

	
		Of interest to this class is the Laplace transform ($\Laplace{\hspace{1ex}}$) of the function $f(t)$, expressed as $\Laplace{f(t)}$. Here, a Laplace transform is used as a method of solving the differential equations of motion by reducing the computation needed to that of integration and algebraic manipulation. 
		
		The definition of the Laplace transform of the function $f(t)$ is:
		
		\begin{equation}
				\Laplace{f(t)} = F(s) = \int_{0}^{\infty} f(t)e^{-st}dt
		\end{equation}
		where $s$ represents a variable in the complex plane (also called the $s$-plane) and $f(t)=0$ for all values of $t<0$. Here, the $s$ is a complex value. Lastly, the term $F(s)$ is a generic term that  represents the input to a system. As this class needs the derivatives of the base function, we will calculate these next:
		\begin{equation}
			\Laplace{\dot{f}(t)} = \int_{0}^{\infty} \dot{f}(t)e^{-st}dt = \int_{0}^{\infty} e^{-st}\frac{d[f(t)]}{dt}dt 
		\end{equation}		
		integration by parts yields:
		\begin{equation}
			\Laplace{\dot{f}(t)} = e^{-st}f(t)\Big|_0^\infty+s\int_{0}^{\infty}e^{-st}f(t)dt
		\end{equation}
		Astutely, it can be noticed that the second term $s\int_{0}^{\infty}e^{-st}f(t)dt$
		is the input to the system $F(s)$. Therefor, with a little rearranging this becomes:
		\begin{equation}
			\Laplace{\dot{f}(t)} = sF(s)-f(0)
		\end{equation}
		Taking the derivative of again yields:
		\begin{equation}
			\Laplace{\ddot{f}(t)} = s^2F(s)-sf(0)-\dot{f}(0)
		\end{equation}
		
		A few key points of the Laplace transforms are:
				
		\begin{itemize}
			\item The domain of the problem changes from the real number line ($t$) to the complex plane ($s$-plane).
			\item The integration of the Laplace transform changes differentiation into multiplication.
			\item The transform procedure is linear. Therefore, the transform of the linear combination of two transforms is the same as the linear transformation of these functions. 
			\item To move from the time domain to the complex number plane we typically use tables of pre-solved integral. 
			\item The function $x(t)$ can be obtained by taking the inverse Laplace transform defined as $x(t) = \Laplace{X(s)}^{-1}$
		\end{itemize}

			The Laplace transform can be calculated in symbolic form. In particular interest to this text is the Laplace form of the system input $F(s)$ and output $X(s)$. To expand the symbolic form of the Laplace transform for the system inputs are 
			and for system outputs:
			\begin{equation}
				\label{eq:laplace_f}
				\Laplace{f(t)} = F(s)
			\end{equation}		
			\begin{equation}
				\label{eq:laplace_f'}
				\Laplace{\dot{f}(t)} = sF(s)-f(0)
			\end{equation}	
			\begin{equation}
				\label{eq:laplace_f''}
				\Laplace{\ddot{f}(t)} = s^2F(s)-sf(0) - \dot{f}(0)
			\end{equation}	
			here, $f(0)$ and $\dot{f}(0)$ are the initial values of the function $f(t)$.  Furthermore, the for system outputs are:
			\begin{equation}
				\label{eq:laplace_x}
				\Laplace{x(t)} = X(s)
			\end{equation}		
			\begin{equation}
				\label{eq:laplace_x'}
				\Laplace{\dot{x}(t)} = sX(s)-x(0)
			\end{equation}	
			\begin{equation}
				\label{eq:laplace_x''}
				\Laplace{\ddot{x}(t)} = s^2X(s)-sx(0) - \dot{x}(0)
			\end{equation}	
			here, $x(0)$ and $\dot{x}(0)$ are the initial values of the function $x(t)$. 		
	
\end{review}


		\subsection{Transfer Function Method for Solving Vibrating Systems}
		
			As mentioned in the introduction to this chapter, a variety of systems can be solved for using the transfer function method. The procedure for using the Laplace transform to solve equations of motion expressed as an inhomogeneous ordinary differential equation is:
			\begin{enumerate}
				\item Take the Laplace transform of both sides of the EOM while treating the time derivatives symbolically.
				\item Solve for $X(s)$ in the obtained equation.
				\item Apply the inverse transform $x(t) = \Laplace{X(s)}^{-1}$
			\end{enumerate}
				
			\subsubsection{Free Vibration for Undamped Systems}
			Consider the undamped single-DOF system:
			\begin{figure}[H]
				\centering
				\includegraphics[]{../figures/1-DOF-spring_mass_horizontal.png}
				\caption{A spring mass model of a 1-DOF system.}
			\end{figure}
			\noindent The EOM for this system is a homogeneous differential equation becasue the right-hand side is equal to zero:
			\begin{equation}
				m\ddot{x}(t) + kx(t) = 0 
			\end{equation}
			Here we will leave the ``$(t)$'' for clarity to differentiate the time domain solution from Laplace solution ``$(s)$'' in the $s$-plane, as discussed in review~\ref{sec:Laplace_review}. The EOM can be rewritten in standard form as:
			\begin{equation}
				\ddot{x}(t) + \omega_n^2x(t) = 0 
			\end{equation}
			where the initial conditions at $t=0$ are $x(0)=x_0$ and $\dot{x}(0) = v_0$. Taking the Laplace transforms, in symbolic form using equations \ref{eq:laplace_x} - \ref{eq:laplace_x''}, of both sides of the EOM yields:
			\begin{equation}
				\big[s^2X(s) -sx_0 -v_0 \big] + \big[ \omega_n^2X(s) \big] =0
			\end{equation}
			using equations \ref{eq:laplace_x} and \ref{eq:laplace_x''} from section~\ref{sec:Laplace_review}. Solving for the output of the system $X(s)$ yields:
			\begin{equation}
			X(s) = \frac{sx_0 + v_0}{s^2 + \omega_n^2}
			\end{equation}
			We can expand this form of $X(s)$ to obtain equations listed in our Laplace Transform table:
			\begin{equation}
			X(s) = \frac{sx_0}{s^2 + \omega_n^2} + \frac{v_0}{s^2 + \omega_n^2}\cdot \frac{\omega_n}{\omega_n}
			\end{equation}
			This becomes:
			\begin{equation}
			X(s) = x_0\frac{s}{s^2 + \omega_n^2} + \bigg(\frac{v_0}{\omega_n}\bigg) \cdot \frac{\omega_n}{s^2 + \omega_n^2}
			\end{equation}
			
			Next, using the inverse Laplace transform $x(t) = \Laplace{X(s)}^{-1}]$ and the two following Laplace transforms (\#5 and \#6):
			\begin{equation}
			f(t) \text{ is cos}(\omega t) \text{ when }  F(s) \text{ is } \frac{s}{s^2+\omega^2} 
			\end{equation}
			\begin{equation}
			f(t) \text{ is sin}(\omega t)  \text{ when }  F(s) \text{ is } \frac{\omega}{s^2+\omega^2} 
			\end{equation}
			Therefore, we can obtain the solution for the system output $X(s)$ as:
			\begin{equation}
			x(t) = x_0 \text{cos}(\omega_n t) + \frac{v_0}{\omega_n}\text{sin}(\omega_n t)
			\end{equation}
			
			The same procedure can be used to calculate the under damped and forced responses. However, when calculating these responses the algebraic solution for $X(s)$, $s$ often contains quotients of polynomials. These Polynomial ratios may not be found in simple Laplace tables and must be solved using the method of partial fractions. An example of this procedure can be found in Appendix B of Inman. 

			\subsubsection{Forced Vibration (Impulse) for damped Systems}
		
			Shock loads on mechanical systems represent a very common source of vibration. These short-duration forces are also called called an impulse. An impulse excitation is defined as a force that is applied for a very short, or infinitesimal, length of time. An impulse is a nonperiodic force that is represented by the symbol $\delta$. The response of a system to an impulse load is the same as the system's free response provided that the correct initial conditions are applied. This is illustrated in the following where the applied force $F(t)$ is impulsive in nature (i.e., large magnitude over a very short time).
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.5\textwidth]{../figures/unit_impulse.png}
				\caption{An impulse function with the impulse at $t=0$. }
			\end{figure}
			
			The impulse response function can be solved for analytically, however, we will solve it using the transfer function approach. Here we will consider the under-damped spring-mass system. First, assume that the system is at rest (no initial conditions). Next, we write the EOM as:
			\begin{equation}
			m\ddot{x} +c\dot{x} +kx = \delta(t)
			\end{equation}
			Taking the Laplace transform of both sides of the equation yields 
			\begin{equation}
			m\big(s^2X(s)-sx(0) - \dot{x}(0)\big) + c\big(sX(s)-x(0)\big) +kX(s) =1
			\end{equation}
			note that the $\Laplace{\delta}=1$ per \#1 in the transform table. However, if we assume zero initial conditions (a system at rest when the impulse happens), the equation simplifies to. 
			\begin{equation}
			ms^2X(s) + csX(s) +kX(s) =1
			\end{equation}
			or
			\begin{equation}
			(ms^2 + cs +k)X(s) =1
			\end{equation}
			Solving this equation for $X(s)$:
			\begin{equation}
			X(s) = \frac{1}{m} \cdot \frac{1}{s^2 + 2 \zeta \omega_n s + \omega_n^2}
			\end{equation}
			Again, the mass is extracted to develop a formulation that can be found in the Laplace tables. Setting the constraint that $\zeta<1$ and consulting \#10 in the table for Laplace transforms results in:
			\begin{equation}
			x(t) = \frac{1}{m \omega_d} e^{-\zeta \omega_n t} \text{sin}(\omega_dt)
			\end{equation}
			where this is the general solution for a damped system subjected to an impulse loading function. For the undamped case a solution can be obtained by setting $\zeta=0$. This Results in the following form for the undamped case:
			\begin{equation}
			x(t) = \frac{1}{m \omega_n}\text{sin}(\omega_n t)
			\end{equation}
			Below is a typical response for both a undamped and underdamped 1-DOF system subject to an impulse response at $t=0$ seconds. 
			\begin{figure}[H]
				\centering
				\includegraphics[]{../figures/response_impulse.png}
				\caption{Temporal responses from a underdamped and undamped 1-DOF systems to a impulse response function.}
			\end{figure}


\subsubsection{Arbitrary Inputs to a System}
\label{sec:impulse_inputs}
The time domain response of a system to an arbitrary input force in time can be calculated using a series of impulses as shown in figure \ref{fig:Arbitary_excitation_forces}. This method allows the practitioner to easily calculate the response of an arbitrary input to a system using a single expression executed in a ``for loop''. This type of analysis is often more efficient in terms of programming than more direct methods such as the transfer functions sown in this text. 

\begin{figure}[H]
	\centering
	\includegraphics[]{../figures/Arbitary_excitation_forces.png}
	\caption{Generalized response showing that any signal can be represented as a series of impulse signals. }
	\label{fig:Arbitary_excitation_forces}
\end{figure}



\begin{example}


In testing, an hammer is used to excite a 1-DOF system with an impact (i.e. impulse), however, the hammer ascendantly impacts the system twice. The first impact has a force of 0.2 N, while the second has a force of 0.1 N and happens 0.1 seconds after the first impact. Plot the response for the double impact. The system has the parameters $m$ = 1 kg, $c$ = 0.5 kg/s, k = 4 N/m. 

\noindent\textbf{Solution:} First, we can define the forcing function as:

\begin{equation}
	F(t) = 0.2 \delta (t) + 0.1 \delta(t-\tau)
\end{equation}
where $\tau$ is the offset between the first and second impacts. Next, considering that the unit impulse has a magnitude of 1 we can obtain solutions for the first impact by first writing it's EOM:

\begin{equation}
m\ddot{x}(t) +c\dot{x}(t) +kx(t) =0.2 \delta(t)
\end{equation}
Taking the Laplace transform of both sides of the equation yields 
\begin{equation}
m\big(s^2X(s)-sx(0) - \dot{x}(0)\big) + c\big(sX(s)-x(0)\big) +kX(s) = 0.2
\end{equation}
However, assuming zero initial conditions, the equation simplifies to. 
\begin{equation}
(ms^2 + cs +k)X(s) = 0.2
\end{equation}
Solving this equation for $X(s)$:
\begin{equation}
X(s) = \frac{0.2}{m} \cdot \frac{1}{s^2 + 2 \zeta \omega_n s + \omega_n^2}
\end{equation}
Again, consulting \#10 in the table for Laplace transforms results in:
\begin{equation}
x_1(t) = \frac{0.2}{m \omega_d} e^{-\zeta \omega_n t} \text{sin}(\omega_dt)
\end{equation}
where this is the general solution for a damped system subjected to an impulse loading function. The second impact can now be solved for using the same method. However, now the time $(t)$ must be offset by $(\tau)$ to allow the impact to still be located at $t=0$ in terms of the second impact. This results in:
\begin{equation}
	x_1(t) = \frac{0.2}{m \omega_d} e^{-\zeta \omega_n t} \text{sin}(\omega_dt)
\end{equation}
\begin{equation}
	x_2(t) = \frac{0.1}{m \omega_d} e^{-\zeta \omega_n (t-\tau)} \text{sin}\big(\omega_d(t-\tau)\big)
\end{equation}
Next, using the knowledge that the systems are linear and that the Laplace transform of a linear combination of two transforms is the same as the linear transformation of these functions we can build the piecewise function:

\[
  x(t) = 
  \begin{cases}
\frac{0.2}{m \omega_d} e^{-\zeta \omega_n t} \text{sin}(\omega_dt) & \text{if } t < \tau \\
\frac{0.2}{m \omega_d} e^{-\zeta \omega_n t} \text{sin}(\omega_dt)  + \frac{0.1}{m \omega_d} e^{-\zeta \omega_n (t-\tau)} \text{sin}\big(\omega_d(t-\tau)\big) & \text{if } \tau \leq t 
  \end{cases}
\]


For the mass, damping, and stiffness values given above this can be plotted as:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{../figures/response_double_impact.png}
\end{figure}

\end{example}




		\subsection{Laplace method in Controls}

		Again, consider the Laplace system:
		\begin{equation}
				\Laplace{f(t)} = F(s) = \int_{0}^{\infty} f(t)e^{-st}dt
		\end{equation}
		where:
		\begin{equation}
			\int_{0}^{\infty} = \lim_{p \rightarrow \infty} \int_{0}^{p}
		\end{equation}
		exists. For now, $s \in \mathbb{R}$ (real). Later, we will let $s \in \mathbb{C} $ (complex). For any given $F(s)$, we can find the original $F(t)$ using the pairing method. 
		\begin{equation}
			f(t) \overset{LT}{\underset{ILT}\rightleftarrows} F(s) 
		\end{equation}
		this can also be written as:
		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			F(s) & \Laplace{f(t)} \\
			f(t) & \Laplace{F(s)}^{-1} \\
			\end{cases}
		\end{equation}

		\subsubsection{Laplace transform of exponential function}

		Consider the exponential function,
		\begin{equation}
		\Laplace{e^{p_0t}} = \frac{1}{s -p_0} 
		\end{equation}
		where
		$f(t) = e^{p_0t}$ and $F(s) = \frac{1}{s -p_0} $. As $F(s) \rightarrow \infty $, $s \rightarrow p_0$, therefore $p_0$ is a pole of the system. This is shown in Fig.~\ref{fig:S_space_exp_pt}.	

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & e^{p_0t} \\
			F(s) & \frac{1}{s -p_0} \\
			\end{cases}
		\end{equation}

	
		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_space_exponential_function.png}
			\caption{Function $e^{p_0t}$ with $p_0=5$; showing the (a) time domain and; (c) the s-space.}
			\label{fig:S_space_exp_pt}
		\end{figure}



\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{e^{p_0t}} = \frac{1}{s -p_0} 
\end{equation}
Again, consider the Laplace system:
\begin{equation}
		\Laplace{f(t)} = F(s) = \int_{0}^{\infty} f(t)e^{-st}dt
\end{equation}
therefore, 
\begin{equation}
	\int_{0}^{\infty}f(t)e^{-s t} dt =  \int_{0}^{\infty}e^{p_0t}e^{-st}dt = \int_{0}^{\infty}e^{-(s-p_0)t}dt
\end{equation}
Change of variables can be used such that:
\begin{align}
(s-p_0)t &= t^*  \\ \nonumber
(s-p_0)dt &= dt^*  \\ \nonumber
dt &= \frac{1}{s-p_0}dt^*
\end{align}
therefore
\begin{align}
	\int_{0}^{\infty}e^{-(s-p_0)} dt &=  \int_{0}^{\infty}e^{-t^*}\frac{1}{s-p_0}dt^*  \\ \nonumber
	&= \frac{1}{s-p_0} \int_{0}^{\infty}e^{-t^*} dt^*  \\ \nonumber
&= \frac{1}{s-p_0} \big(-e^{-t^*}\big) \bigg|^\infty_0 \\ \nonumber
&= \frac{-1}{s-p_0} \big(e^{-t^*}\big) \bigg|^\infty_0 \\ \nonumber
&= \frac{-1}{s-p_0} \big[e^{-\infty - e^0}\big] \\ \nonumber
&= \frac{-1}{s-p_0} (-1) \\ \nonumber
&= \frac{1}{s-p_0}
\end{align}
Therefore, it can be shown that:
\begin{equation}
\Laplace{e^{p_0t}} = \frac{1}{s -p_0} 
\end{equation}
\end{mdframed}



		\subsubsection{Laplace transform of step function}

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & 1(t) , \; \; \; t>0 \\
			F(s) & \frac{1}{s} \\
			\end{cases}
		\end{equation}


		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_space_step_function}
			\caption{Step function; showing the (a) time domain and; (c) the s-space.}
			\label{fig:T_and_S_space_step_function}
		\end{figure}


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{1(t)} = \frac{1}{s}
\end{equation}
This can be done as:
\begin{align}
	\Laplace{1(t)} &=  \int_{0}^{\infty}1 \cdot e^{-st} dt \\ \nonumber
	&= \int_{0}^{\infty}e^{-st} dt  \\ \nonumber
&= \frac{-1}{s}e^{-st}\bigg|^\infty_0 \\ \nonumber
&= \frac{1}{s} 
\end{align}
\end{mdframed}

		\subsubsection{Laplace transform of ramp function}

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & t, \; \; \; t>0 \\
			F(s) & \frac{1}{s^2} \\
			\end{cases}
		\end{equation}

		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_space_ramp_function}
			\caption{Ramp function; showing the (a) time domain and; (c) the s-space.}
			\label{fig:T_and_S_space_ramp_function}
		\end{figure}





\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{t} = \frac{1}{s^2}
\end{equation}
This can be done as:
\begin{equation}
\Laplace{t} =  \int_{0}^{\infty}t \cdot e^{-st} dt
\end{equation}
integration by parts leads to:
\begin{equation}
d[uv] = udv + vdu \rightarrow vdu = d[uv] - udv
\end{equation}
where
\begin{equation}
 \int_{a}^{b} vdu = uc \bigg|_a^b - \int_{a}^{b} u dv
\end{equation}
which leads to:
\begin{equation}
du = e^{-st};  \; \; \; u = \frac{1}{-s}e^{-st}
\end{equation}
or more simply, 
\begin{equation}
du = 1;  \; \; \; u = t
\end{equation}
Using these expressions, we get
\begin{align}
	 \int_{0}^{\infty}t \cdot e^{-st} dt &=  \frac{t}{-s} e^{-s t} \bigg|_0^\infty - \int_{0}^{\infty}\frac{-1}{-s}e^{-st} dt \\ \nonumber
	&= \frac{1}{s} \int_{0}^{\infty} e^{-st} dt \\ \nonumber
&= \bigg[\frac{1}{s}\bigg]\bigg[\frac{1}{s}\bigg],\text{ as }  \Laplace{1(t)} =  \int_{0}^{\infty} e^{-st} dt  = \frac{1}{s} \\ \nonumber
&= \frac{1}{s^2}
\end{align}
\end{mdframed}


		\subsubsection{Laplace transform shifted step function}

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & 1(t-\tau) \\
			F(s) & e^{-\tau s} \frac{1}{s} \\
			\end{cases}
		\end{equation}

		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_shifted_space_step_function}
			\caption{Shifted step function; showing the (a) time domain and; (c) the s-space. }
			\label{fig:Laplace_shifted_step_transform}
		\end{figure}

\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{1\cdot(t-\tau)} =e^{-\tau s} \frac{1}{s}
\end{equation}
This can be done as:
\begin{equation}
\end{equation}
\begin{align}
	 \Laplace{1\cdot(t-\tau)}  &=  \int_{0}^{\infty} 1 (t-\tau) e^{-st} dt \\ \nonumber
	&=  \int_{0}^{\tau} 0 \cdot  e^{-st} dt + \int_{\tau}^{\infty} e^{-st} dt  \\ \nonumber
	&=  \int_{\tau}^{\infty} e^{-st} dt  \\ \nonumber
\end{align}
using a change of variable substitution, 
\begin{equation}
t^* = t-\tau; \; \; \; t = t^* + \tau; \; \; \; dt^* = dt
\end{equation}
Therefore
\begin{align}
	 \int_{\tau}^{\infty} e^{-st} dt  &=  \int_{0}^{\infty}e ^ {-s (t^* + \tau)} dt^* \\ \nonumber
	&=  e^{-s \tau}  \int_{0}^{\infty}e ^ {-s (t + \tau^*)} dt^*   \\ \nonumber
	&=  e^{-s \tau} \frac{1}{s}  \\ \nonumber
\end{align}
connecting the start of proof to the end shows:
\begin{equation}
\Laplace{1\cdot(t-\tau)} =e^{-\tau s} \frac{1}{s}
\end{equation}
\end{mdframed}


		\subsubsection{Laplace transform pulse function}

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & p(t; \; \; \tau) \\
			F(s) & \frac{1-e^{-s \tau}}{s \tau}
			\end{cases}
		\end{equation}

		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_pulse_function}
			\caption{Pulse function; showing the (a) time domain and; (c) the s-space.}
			\label{fig:Laplace_pulse_transform}
		\end{figure}


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{p(t;\tau)} = \frac{1-e^{-st\tau}}{s \tau}
\end{equation}
This can be done by writing the pulse as a step up followed by a step down at $t=\tau$ and scaled to be $\frac{1}{\tau}$, i.e.,:
\begin{equation}
p(t;\tau) = \frac{1}{\tau}1(t) - \frac{1}{\tau} 1 (t-\tau)
\end{equation}
given that
\begin{equation}
\Laplace{1(t)} =  \frac{1}{s}, \; \; \; \Laplace{1 (t-\tau)} = e^{-s \tau} \frac{1}{s}
\end{equation}
the $s$ domain of $p(t;\tau)$ is expressed as
\begin{equation}
F(s) =   \frac{1}{\tau}  \frac{1}{s}  - \frac{1}{\tau} e^{-s \tau} \frac{1}{s}
\end{equation}

\begin{align}
	 F(s) &=   \frac{1}{\tau}  \frac{1}{s}  - \frac{1}{\tau} e^{-s \tau} \frac{1}{s} \\ \nonumber
	&=  \frac{1-e^{-s \tau}}{s \tau} \\ \nonumber
\end{align}
connecting the start of proof to the end shows:
\begin{equation}
\Laplace{p(t;\tau)} = \frac{1-e^{-s \tau}}{s \tau}
\end{equation}

\end{mdframed}



		\subsubsection{Laplace transform impulse function}

		\begin{equation}
		\text{LT pair} =
			\begin{cases}
			f(t) & \delta(t) \\
			F(s) & 1 \\
			\end{cases}
		\end{equation}

		\begin{figure}[H]
			\centering
			\includegraphics[width=6.5in]{../figures/T_and_S_impulse_function}
			\caption{Impulse function; showing the (a) time domain and; (c) the s-space.}
			\label{fig:T_and_S_impulse_function}
		\end{figure}


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
Show that
\begin{equation}
\Laplace{\delta(t)} = 1
\end{equation}
Consider $\delta(t)$ as the limit of $p(t;\tau) $ as $\tau \rightarrow 0$ and take the Laplace Transform. 
\begin{equation}
\delta(t) = \lim\limits_{\tau \rightarrow 0} p(t;\tau) 
\end{equation}
taking the Laplace transform
\begin{align}
\Laplace{\delta(t)} &= \lim\limits_{\tau \rightarrow 0} \Laplace{p(t;\tau)} \\ \nonumber
&=  \lim\limits_{\tau \rightarrow 0} \frac{1-e^{-st}}{s \tau}
\end{align}
taking the limit of this in ${1-1}/0 = 0/0$. Applying L'Hospital's (loh-pee-TAHL) rule, 
\begin{align}
\lim\limits_{\tau \rightarrow 0} \frac{ \frac{\partial}{\partial \tau} \big(     1-e^{-s t}    \big)  }{ \frac{\partial}{\partial \tau} (s \tau)} &= \lim\limits_{\tau \rightarrow 0} \frac{- \big(-s e^{-s \tau \big)}}{s}  \\ \nonumber
&=  \lim\limits_{\tau \rightarrow 0} e^{-s \tau} \\ \nonumber
&= 1
\end{align}
connecting the start of proof to the end shows:
\begin{equation}
\Laplace{\delta(t)} = 1
\end{equation}

Moreover, this can also be done useint the localization property of the delta function. Recall that
\begin{equation}
\int_{0}^{\infty} \delta(t) g(t) dt = g(0)
\end{equation}
then
\begin{align}
\Laplace{\delta(t)} &= \int_{0}^{\infty} \delta(t) e^{-s t} dt; \text{ where } g(t) = e^{-s t} \\ \nonumber
&= e^{-s t}\big|_{t=0} \\ \nonumber
&= e^0 \\ \nonumber
&= 1
\end{align}
Again, connecting the start of proof to the end shows:
\begin{equation}
\Laplace{\delta(t)} = 1
\end{equation}
\end{mdframed}

\subsection{Properties of Laplace Transforms}


There are a variety of Laplace transforms that can assist in moving between $f(t)$ and $F(s)$. Importantly, the differentiation, integration, time shift, and final value theorems are useful as they allow you to expand a Laplace table by taking the Laplace transform of functions without using the Laplace transform definition. Moreover, these properties are used extensively during control system design so being fluent in them makes you a better designer. A subset of these are shown in Table~\ref{table:laplace_theorems} while some of the more important ones are derived in the following sub sections. 

The goal of studying the Laplace transform properties is to be able to Laplace Transform a differential equation with initial conditions and to relate differentiation and integration in the time domain to their counterparts in the s-domain domain. 

\begin{table}[H]
\centering
\caption{A list of important Laplace Transform theorems used in control.}
\label{table:laplace_theorems}
\begin{tabular}{@{}lll@{}}
\toprule
number & theorem & name \\ \midrule
1 & $ 	\Laplace{f(t)} = F(s) = \int_{0}^{\infty} f(t)e^{-st}dt$ & definition \\
2 & $ 	\Laplace{kf(t)} = kF(s) $ & linearity theorem \\
3 & $ \Laplace{f_1(t)+f_2(t)} = F_1(s) + F_2(s) $ & linearity theorem \\
4 & $ \Laplace{e^{-at}f(t)} = F(s+a)$ & frequency shift theorem \\
5 & $ \Laplace{f(t-t_0)} = e^{-t_0s}F(s) $ & time shift theorem \\
6 & $ \Laplace{e^{s_0t}f(t)} = F(s-s_0)$ & time shift theorem \\
7 & $ \Laplace{f(at)} = \frac{1}{a}F \frac{s}{a} $ & scaling theorem \\
8 & $\Laplace{f'(t)} = sF(s) \text{, if } f(0) = 0   $ & differentiation theorem \\
9 & $ \Laplace{f''(t)} = s^2F(s) \text{, if } f'(0) = f(0) = 0  $ & differentiation theorem \\
10 & $ \Laplace{f^{n}(t)} = s^{n}F(s) \text{, if } f^{n-1}(0) = \cdots = f'(0) = f(0) = 0 $ & differentiation theorem \\
11 & $ \Laplace{\int_{0}^{t}f(t^*)dt^*} = \frac{1}{s}F(s) $ & integration theorem\\
12 & $ f(\infty)  = \lim\limits_{s\rightarrow 0} [s F(s)] $ & final value theorem$^1$ \\
13 & $ f(0+)  = \lim\limits_{s\rightarrow \infty} [s F(s)] $ & initial value theorem$^2$ \\ \bottomrule
\end{tabular}
\end{table}
\vspace{-2ex}
\noindent {\small $^1$This requires all roots of the denominator of $F(s)$ to have negative real parts, and no more than one can be on the origin of the real axis.} \\
\noindent {\small$^2$This requires that $f(t)$ be continuous or have a step discontinuity at $t=0$. This does not allow for impulses at $t=0$.}




\subsubsection{Differentiation Property}

The Laplace transforms can be simplified if we assume $ f(0) = 0$, where these are the terms associated with the initial conditions. This is shown as:
\begin{align}
\Laplace{f'(t)} &= sF(s) \text{, if } f(0) = 0   \\
\Laplace{f''(t)} &= s^2F(s) \text{, if } f'(0) = f(0) = 0  \nonumber \\
&\vdots  \nonumber \\
\Laplace{f^{(n)}(t)} &= s^{(n)}F(s) \text{, if } f^{(n-1)}(0) = \cdots = f'(0) = f(0) = 0 \nonumber  
\end{align}
Recall that differentiation in the time domain becomes multiplication by s in the s-domain.


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}

\noindent \textbf{Method 1:}
Show that $\Laplace{f'(t)} = s F(s)$ if $f(0) = 0$;
\begin{equation}
\Laplace{f'(t)} = \int_{0}^{\infty} f'(t)e^{-st}dt 
\end{equation}
integration by parts $udv = d(uv) -vdu$ where
\begin{align}
e^{-st} &= u   \\
f'(t)dt &= dv  \nonumber \\
f(t) &= v  \nonumber \\
-se^{-st} &= du   \nonumber
\end{align}
leads to
\begin{equation}
\Laplace{f'(t)} = f(t)e^{-st}\big|_{0}^{\infty} - \int_{0}^{\infty}f(t)(-se^{-st})dt
\end{equation}
further simplification results in:
\begin{align}
\Laplace{f'(t)} &= f(\infty)e^{- \infty} - f(0) + s \int_{0}^{\infty} f e^{-s t} dt   \\
 &= s F(s) -f(0)  \nonumber \\
 &= s F(s)  \nonumber
\end{align}
where $F(s) = \int_{0}^{\infty} f e^{-s t} dt$, $e^{- \infty}=0$, and $f(0) = 0$. Therefore, 
\begin{equation}
\Laplace{f'(t)} = s F(s)
\end{equation}

\noindent \textbf{Method 2:}
Show that $\Laplace{f'(t)} = s F(s)$ if $f(0) = 0$. Denote $f(t) = f'(t)$, therefore:
\begin{align}
G(s) &= \Laplace{g(t)} = \Laplace{f'(t)} = sF(s)  \\
\Laplace{f''(t)} &= \Laplace{g'(t)} = s\Laplace{G(s)} = s^2F(s)  \nonumber
\end{align}
by induction it can be shown that 
\begin{equation}
\Laplace{f'(t)} = s F(s)
\end{equation}

Bottom line: ``to differentiate, multiply by $s$'', provided $f(0) =0$, $f'(0) =0$, etc. Else, need to subtract them $\Laplace{f'(t)} = sF(s) -f(0)$ etc. 
\end{mdframed}

\subsubsection{Integration Property}

The integral of $f(t)$ is
\begin{equation}
\Laplace{\int_{0}^{t}f(t^*)dt^*} = \frac{1}{s}F(s)
\end{equation}
where $t^*$ is the dummy variable to integrate over as $t$ is used for the limit.  The main take away here is that integration in the time domain becomes division by s in the s-domain.

\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}

\noindent Show that $\Laplace{\int_{0}^{t}f(t^*)dt^*} = \frac{1}{s}F(s)$. Denote $g(t) =  \int_{0}^{t}f(t^*)dt^*$, then:
\begin{equation}
g'(t) = f(t)\text{; } g(0) = 0
\label{eq:laplace_integration_property}
\end{equation}
taking the Laplace of the left hand side of this expression yields:
\begin{equation}
\Laplace{g'(t)} = s\Laplace{g(t)} = s \Laplace{\int_{0}^{t}f(t^*)dt^*}
\end{equation}
the Laplace of the right hand side of this expression yields:
\begin{equation}
\Laplace{f(t)} = F(s)
\end{equation}
therefore, by equation~\ref{eq:laplace_integration_property} 
\begin{equation}
s \Laplace{\int_{0}^{t}f(t^*)dt^*} = F(s)
\end{equation}
divide by $s$ to get 
\begin{equation}
\Laplace{\int_{0}^{t}f(t^*)dt^*} = \frac{1}{s}F(s)
\end{equation}

Bottom line: ``to integrate, divide by $s$''. 
\end{mdframed}


\subsubsection{Time Shift Property}
%\todo{should this be called $\tau$ and not $t_0$?}
To shift in $t$
\begin{equation}
\Laplace{f(t-t_0)} = e^{-t_0s}F(s)
\end{equation}
To shift in $s$
\begin{equation}
\Laplace{e^{s_0t}f(t)} = F(s-s_0)
\end{equation}
A shift in the $t$ domain
		\begin{figure}[H]
			\centering
			\includegraphics[width=3.5in]{../figures/shift_properties}
			\caption{Shift of a signal in the $t$ domain.}
			\label{fig:shift_properties}
		\end{figure}
Note that:
\begin{itemize}[noitemsep, topsep=0pt]
\item Function $f(t)$ is zero for $t<0$ arrangement
\item Function $f(t-t_0)$ is zero for $t<t_0$ arrangement
\end{itemize}


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}
\noindent \textbf{t-domain}

For a shift in $t$, show that $\Laplace{f(t-t_0)} = e^{-t_0s}F(s)$
\begin{align}
\Laplace{f(t-t_0)} &= \int_0^\infty f(t-t_0) e^{-st}dt  \\
&= \int_0^{t_0} 0 \cdot e^{-st}dt + \int_{t_0}^\infty f(t-t_0) e^{-st}dt  \nonumber \\
&= \int_{t_0}^\infty f(t-t_0) e^{-st}dt  \nonumber
\end{align}
using a change of variables:
\begin{align}
t* &= t-t_0 \rightarrow t = t*+t_0\\
dt* &= dt  \nonumber
\end{align}
therefore, 
\begin{align}
\Laplace{f(t-t_0)} &= f(t^*)e^{-s(t^*+t_0)}dt^*     \\
&= e^{-s t_0} \int_0^{\infty} f(t^*)e^{-s t^*}dt^*  \nonumber \\
&= e^{-s t_0} F(s)  \nonumber
\end{align}
lastly, we can show that:
\begin{equation}
\Laplace{f(t-t_0)} = e^{-t_0s}F(s)
\end{equation}

\noindent \textbf{s-domain}

For a shift in $s$, show that $\Laplace{e^{s_0t}f(t)} = F(s-s_0)$
\begin{align}
\Laplace{e^{s_0t}f(t)} &= \int_0^{\infty} e^{-s_0 t}  f(t)e^{-s t}dt  \nonumber \\
&=\int_0^{\infty} f(t)e^{-(s-s_0) t}dt  \nonumber \\
&= F(s-s_0)
\end{align}

\end{mdframed}

 


\subsubsection{Final Value Theorem}
This applies to the steady state response $x(\infty)$ which can also be denoted with a subscript ss as $x_{ss}$ where

\begin{equation}
x_{ss} = x(\infty)  = \lim\limits_{t\rightarrow \infty} [x(t)]  =  \lim\limits_{s\rightarrow 0} [s X(s)]
\end{equation}
therefore, 
\begin{equation}
x_{ss} =  \lim\limits_{s\rightarrow 0} [s X(s)]
\end{equation}


\begin{mdframed}[middlelinewidth=0.5mm]
\begin{center}
\gr{Proof}
\end{center}

Prove that $x_{ss} = \lim\limits_{s\rightarrow 0} s X(s)$. Starting with, 
\begin{equation}
\Laplace{\dot{x}(t)} = s X(s)\text{, } x(0)=0
\end{equation}
by definition
\begin{equation}
\Laplace{\dot{x}(t)} = \int_0^{\infty} \dot{x}(t)e^{-s t}dt
\end{equation}
combing the last two equations yields
\begin{equation}
\int_0^{\infty} \dot{x}(t)e^{-s t}dt = s X(s)
\end{equation}
taking the limit as $s \rightarrow 0$ results in 
\begin{equation}
\lim\limits_{s \rightarrow 0} \bigg[ \int_0^{\infty} \dot{x}(t)e^{-s t}dt \bigg] = \lim\limits_{s \rightarrow 0}[s X(s)]
\label{eq:final_value_theorem_eq_1}
\end{equation}
but
\begin{equation}
\lim\limits_{s \rightarrow 0} [ e^{-s t}] = e^0 = 1
\end{equation}
therefore, the left hand side of equation~\ref{eq:final_value_theorem_eq_1} simplifies to 
\begin{equation}
\int_0^{\infty} \dot{x}(t)dt = x(t)  \big|_{0}^{\infty} = x(\infty) = x_{ss}
\end{equation}
inserting this term back into equation~\ref{eq:final_value_theorem_eq_1} yields the desired proof
\begin{equation}
x_{ss} =  \lim\limits_{s\rightarrow 0} [s X(s)]
\end{equation}

\end{mdframed}


\subsection{Convolutional property of Laplace Transforms}



\begin{align}
\label{eq:convolutional_property_of_laplace_transforms}
\Laplace{G(s) \cdot F(s)}^{-1} &= \Laplace{F(s) \cdot G(s)}^{-1} \\
&= \int_0^t f(\tau) g(t-\tau) d \tau \nonumber \\
&= (f * g)(t)  \nonumber \\ 
&= x(t) \nonumber
\end{align}
where $f(\tau)$ is the excitation function and $g(t-\tau)$ is the impulse response of the function. Convolution expresses the system response $x(t)$ to a complicated excitation $f(\tau)$ as an integral using the impulse response $g(t)$  shifted by $\tau$ to $t-\tau$. 
The integral in equation~\ref{eq:convolutional_property_of_laplace_transforms} is not easily computed in the time domain, however, Laplace transforms make it easy using a three step process:
\begin{enumerate}[noitemsep,topsep=0pt]
\item Calculate $F(s) = \Laplace{f(t)} $, $G(s) = \Laplace{g(t)}$
\item Multiply $F(s)G(s)$
\item Take the inverse Laplace transform to get $x(t)$
\end{enumerate} 

\subsection{Transfer Function for Response to Random Inputs}

			Consider the following system
			\begin{figure}[H]
				\centering
				\includegraphics[]{../figures/transfer_function_system.png}
				\caption{Generic block diagram of a system $H(s)$ subjected to an input $F(s)$ and its corresponding output $X(s)$ where the $(s)$ denotes that the considered system is in the $s$-plane.}
				\label{fig:transfer_function_system}
			\end{figure}
			\noindent where $F(s)$ is the input, $H(s)$ is the system, and $X(s)$ is the output from the system. This formulation is called the transfer-function approach and is commonly used for the formulation and solution of dynamic problems in the control literature. It can also be used for solving the various forced-vibration problems including those from complex or stochastic inputs. 
	
\subsubsection{Defining the transfer function $\mathbf{H(s)}$}

Again, consider the generic system represented in figure~\ref{fig:transfer_function_system}. For this system representation, $F(s)$ is the Laplace of the transform of the driving force and $H(s)$ is the Laplace transform of the response of the system $h(t)$. 

We need to define transfer function $H(s)$ for a generic system. To do this let us show the reasoning behind the transfer function. Here we will show that the output of any system ($x(t)$) can be related to the input of the system ($f(t)$) through a series of polynomial coefficients ($a$ and $b$). Consider the general $n^{th}$-order linear, time-invariant differential equation that governs the behavior of the dynamic system.

\begin{equation}
a_n\frac{d^nx(t)}{dt^n} + a_{n-1}\frac{d^{n-1}x(t)}{dt^{n-1}} + ... + a_0x(t) = b_m\frac{d^mf(t)}{dt^m} + b_{m-1}\frac{d^{m-1}f(t)}{dt^{m-1}} + ... + b_0f(t)
\end{equation} 
where $x(t)$ is the output and $f(t)$ is the input. Note that this is similar to the formulation we have had before for the EOM. Taking the Laplace transformation of both side of the above equation yields

\begin{eqnarray}
&a_ns^nX(s) + a_{n-1}s^{n-1}X(s) + ... + a_0X(s) + \text{initial condition for } x(t) =   \\
&b_ms^mF(s) + b_{m-1}s^{m-1}F(s) + ... + b_0F(s) + \text{initial condition for } f(t)  \nonumber
\label{eq:transfer_function_polynominal_expansion}
\end{eqnarray}
It can be seen that this equation is a purely algebraic expression. If we assume the initial conditions to be zero, the equation reduces to the following:
\begin{eqnarray}
(a_ns^n + a_{n-1}s^{n-1} + ... + a_0)X(s) =  (b_ms^m + b_{m-1}s^{m-1} + ... + b_0)F(s) 
\label{eq:transfer_algebraic_expression}
\end{eqnarray}
if we rearrange equation \ref{eq:transfer_algebraic_expression} to solve for the relationship between the Laplace variables $\big( X(s)$ and $F(s) \big)$ and the algebraic expressions we get:
\begin{equation}
\frac{X(s)}{F(s)} = \frac{b_ms^m + b_{m-1}s^{m-1} + ... + b_0}{a_ns^n + a_{n-1}s^{n-1} + ... + a_0}
\end{equation}
this shows that the ratio of the input algebraic expressions over the output algebraic expressions is equal to the ratio of the output Laplace variable over the input Laplace variable. This shows that we can relate the Laplace variables to the algebraic expressions. Therefore, we can define the transfer function $H(s)$ as: 
\begin{equation}
G(s) = H(s) = \frac{X(s)}{F(s)}
\label{eq:transfer_function}
\end{equation}

In a more formal term, the transfer function that is defined as: ``The ratio of the Laplace transforms of the output or response function to the Laplace transform of the input or forcing function assuming zero initial conditions''. Note that many texts define $H(s)$ as $G(s)$ and this is simply a matter of syntax. 

Equation \ref{eq:transfer_function} can be rearranged to show that the output of the system $X(s)$, can be obtained if we know the input $F(s)$ and the transfer function $H(s)$:
\begin{equation}
 X(s) = H(s)F(s)
\end{equation}	


\subsubsection{Transfer Function method (Steady-State solution)}

Considering the forced system:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{../figures/1-DOF-spring_dashpot_mass_horizontal_forced.png}
	\caption{A spring-dashpot-mass model of a 1-DOF system with external excitation.}
\end{figure}
\noindent that can be expressed as the equation of motion
\begin{equation}
	m\ddot{x}(t) + c\dot{x}(t) +kx(t) = F_0 \cos(\omega t)
\end{equation}
Here $F_0 \cos(\omega t)$, is used at the input but any input will develope the same transfer function as the transfer function is bounded to the system and not the input. From the \#6 in the table for Laplace Transforms, we know that
\begin{equation}
	\Laplace{\cos(\omega t)} = \frac{s}{s^2+\omega^2}
\end{equation}
Therefore, 
\begin{equation}
F(s) = \frac{F_0s}{s^2+\omega^2}
\end{equation}
Ignoring the initial conditions, and therefore considering only the particular solution, and taking the Laplace transform of the EOM equation yields:
\begin{equation}
(ms^2 + cs +k)X(s) = \frac{F_0s}{s^2+\omega^2} 
\end{equation}
where $X(s)$ denotes the Laplace transform of the unknown function $x(t)$ and $s$ is the complex transform variable. Rearranging the above equation for $X(s)$ yields: 
\begin{equation}
X(s) = \frac{F_0s}{(ms^2 + cs +k)(s^2+\omega^2)}
\end{equation}
Now that we have $F(s)$ and $X(s)$ we can obtain $H(s)$ as  
\begin{equation}
H(s) = \frac{X(s)}{F(s)} = \frac{F_0s}{(ms^2 + cs +k)(s^2+\omega^2)} \cdot \frac{s^2+\omega^2}{F_0s} = \frac{1}{ms^2+cs+k}
\end{equation}
or 
\begin{equation}
H(s) = \frac{1}{ms^2+cs+k}
\end{equation}
This ratio is termed the transfer function of a system and is an important tool in vibration analysis.

Sometimes, how the system responds to a inputs with certain frequency components is important in understanding they system in general, therefore, we want to solve for the frequency response function of the system. The frequency response function is denoted as $H(j\omega)$ where the complex number $s$ is replaced by the frequency component of the system while considering the imaginary portion in the complex plane (i.e., $s = j\omega$). Therefor, the frequency response function of the system becomes:

\begin{equation}
H(j\omega) = \frac{1}{m(j\omega)^2+cj\omega+k} = \frac{1}{-m\omega^2+cj\omega+k} 
\end{equation}
rearranging into a standard form yields:
\begin{eqnarray}
H(j\omega) = \frac{1}{k-m\omega^2+c\omega j}
\label{eq:frequency_response_function}
\end{eqnarray}
recall that $j^2=-1$. This is the frequency response function of the system. Therefore, it can be seen that the frequency response function of the system is the transfer function of the system evaluated along the imaginary axis $s=j\omega$. However, this expression contains imaginary values (that help to account for the phase in the system) and therefore can be challenging to work with. As the amplitude $|H(j\omega)|$ of the response (the real portion of the equation) is useful to the practitioner, it is prudent to consider the special case of amplitude response while neglecting the phase response. Consider that:
\begin{equation}
	H(j\omega) = \text{R}+\text{I}j
\end{equation}  
so
\begin{equation}
	 |H(j\omega)|=\sqrt{\text{R}^2+\text{I}^2}
\end{equation}  
multiplying $H(j\omega)$ by 1 that is represented by the its unit complex conjugate yields:
\begin{align}
H(j\omega) &= \bigg( \frac{1}{k-m\omega^2+c\omega j} \bigg) \bigg( \frac{k-m\omega^2-c\omega j}{k-m\omega^2-c\omega j}\bigg)  \\
&= \bigg( \frac{k-m\omega^2}{(k-m\omega^2)^2(c\omega)^2} \bigg) \bigg( \frac{-c\omega}{(k-m\omega^2)^2(c\omega)^2}j\bigg)  \nonumber
\end{align}
therefore, $\text{R} = \frac{k-m\omega^2}{(k-m\omega^2)^2(c\omega)^2} $ and $\text{I} = \frac{-c\omega}{(k-m\omega^2)^2(c\omega)^2}$. Now, calculating the amplitude of $H(j\omega)$ we get:
\begin{align}
H(\omega) &= |H(j\omega)|  \\
&=  \sqrt{\text{R}^2+\text{I}^2} \nonumber  \\
&=  \sqrt{\frac{(k-m\omega)^2+(-c\omega)^2}{\big((k-m\omega^2)^2+(c\omega)^2)\big)^2}}  \nonumber \\
&=  \sqrt{\frac{1}{(k-m\omega^2)^2+c^2\omega^2}}  \nonumber \\
&= \frac{1}{\sqrt{(k-m\omega^2)^2+c^2\omega^2}}  \nonumber
\end{align}
where $H(\omega)$ represents only the amplitude of the frequency response function and therefore drops the $j$ term from the expression. 



To recap, for a single DOF damped spring-mass system the transfer function is:
\begin{equation}
H(s) = \frac{1}{ms^2+cs+k}
\end{equation}
And the frequency response function is:
\begin{eqnarray}
H(j\omega) = \frac{1}{k-m\omega^2+c\omega j}
\end{eqnarray}
While the amplitude of the frequency response is:
\begin{eqnarray}
H(\omega) = |H(j\omega)| = \frac{1}{\sqrt{(k-m\omega^2)^2+c^2\omega^2}}
\end{eqnarray}

\begin{example}
	Considering the forced system:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\textwidth]{../figures/1-DOF-spring_dashpot_mass_horizontal_forced.png}
		\caption{A spring-dashpot-mass model of a 1-DOF system with external excitation.}
	\end{figure}
	Set the forcing function to be $F_0 \sin(\omega t)$ and calculate the transfer function. 
	
	\noindent\textbf{Solution:} The equation of motion for the system is:
	\begin{equation}
		m\ddot{x} + c\dot{x} +kx = F_0 \sin(\omega t)
	\end{equation}
	From the \#6 in the table for Laplace Transforms, we know that:
	\begin{equation}
		\Laplace{\sin(\omega t)} = \frac{\omega}{s^2+\omega^2}
	\end{equation}
	Therefore, 
	\begin{equation}
	F(s) = \frac{F_0\omega}{s^2+\omega^2}
	\end{equation}
	Ignoring the initial conditions and taking the Laplace transform of the EOM equation yields:
	\begin{equation}
	(ms^2 + cs +k)X(s) = \frac{F_0 \omega}{s^2+\omega^2} 
	\end{equation}
	Solving algebraically for the $X(s)$ yields: 
	\begin{equation}
	X(s) = \frac{F_0\omega}{(ms^2 + cs +k)(s^2+\omega^2)}
	\end{equation}
	Now that we have $F(s)$ and $X(s)$ we can obtain $H(s)$ as  
	\begin{equation}
	H(s) = \frac{X(s)}{F(s)} = \frac{F_0 \omega }{(ms^2 + cs +k)(s^2+\omega^2)} \cdot \frac{s^2+\omega^2}{F_0 \omega} = \frac{1}{ms^2+cs+k}
	\end{equation}
	or 
	\begin{equation}
	H(s) = \frac{1}{ms^2+cs+k}
	\end{equation}
	This is identical to the solution obtained using $F_0 \cos(\omega t)$ as would be expected because the transfer function is related to the system and not to the input. 
\end{example}  

\begin{review}
	\label{sec:TimeFrequencySpectrum}

	\textbf{Frequency and Time Domains}

	\noindent The frequency domain is a mathematical representation of a signal or data in terms of its frequency components, as opposed to its temporal or time-based representation. The frequency domain provides a different perspective on the signal by decomposing it into its constituent sinusoidal signals at discrete frequencies and their respective magnitudes. A 3D rerensation of this process is shown in figure~\ref{fig:3D_time_frequency_analysis}. The transformation between the time domain and the frequency domain is typically achieved using mathematical techniques such as the Fourier Transform or the Fast Fourier Transform (FFT).

		\begin{figure}[H]
			\centering
			\includegraphics[width=6in]{../figures/3D_time_frequency_analysis}
			\caption{3D visualization of time and frequency domains where a temporal signal is decomposed into constituent sinusoidal signals.}
			\label{fig:3D_time_frequency_analysis}
		\end{figure}

\end{review}


\subsubsection{Response to Random Inputs}
The transfer and frequency response functions can be very useful for determining the system's response to random inputs. Up to this point we have solved for deterministic input. 

\begin{itemize}
\item \textbf{Deterministic}-For a known time $t$, the value of the input force $F(t)$ is precisely known. 
\item \textbf{Random} For a known time $t$, the value of the input force $F(t)$ is known only statistically. 
\end{itemize}

To expand, a random signal is a signal with no obvious pattern. For these types of it is not possible to focus on the details of the input signal, as is done with a deterministic signal, rather the signal is classified and manipulated in terms of its statistical properties. 

Randomness in vibration analysis can be thought of as the result of a series of results obtained from testing a system repeatability for various inputs under varying conditions. In these cases, one record or time history is not enough to describe the system. Rather, an ensemble of various tests are used to describe how the system will respond to the various inputs. 

First, let us consider two inputs, a deterministic input (typical sin wave), and a random input (white noise). These inputs are shown in figure \ref{fig:Response_to_random_input_inputs}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../figures/Response_to_random_input_inputs.png}
	\caption{Two arbitrary inputs: (a) sinusoidal; and (b) uniform random noise.}
	\label{fig:Response_to_random_input_inputs}
\end{figure}

One of the first factors to consider is the mean of the random signal $x(t)$, defined as:
\begin{equation}
E[x] = \bar{x} = \lim\limits_{T \rightarrow \infty} \frac{1}{T} \int_{0}^{T}x(t)dt
\end{equation}
where $T$ is the length in time of the data collected. However, for random signals we often want to consider signals with an average mean of zero (i.e. $\bar{x}(t)=0$). Therefore, for signals not centered around zero we can obtain a zero centered signal if the signal is stationary and we subtract the mean value from $\bar{x}$ from the signal $x(t)$. This can be written as:
\begin{equation}
x'(t) = x(t) - \bar{x}
\end{equation} 
where the $x'(t)$ is now centered around zero. As mentioned before, it is important to consider whether or not the input signals are stationary. A signal is stationary if its statistical properties (usually expressed by its mean) do not change with time. Here, it can be seen that for our inputs considered the signals are stationary if a long enough time period is considered. 

Another important variable is variance (or mean-square value) of the random variable $x(t)$ defined as:
\begin{equation}
E[(x-\bar{x})^2] = \lim\limits_{T \rightarrow \infty} \frac{1}{T} \int_{0}^{T}(x(t)-\bar{x})^2dt
\end{equation}
and provides a measure of the magnitude of the fluctuations in the signal $x(t)$. If the signal has an expected value of zero, or $E[x]=0$, this simplifies to. 
\begin{equation}
E[x^2] = \overline{x^2} = \lim\limits_{T \rightarrow \infty} \frac{1}{T} \int_{0}^{T}x^2(t)dt
\end{equation}
This expression leads to the calculation of the root-mean-square (RMS) of the signal:
\begin{equation}
x_\text{rms} = \sqrt{\overline{x^2}} 
\end{equation}

Considering a nonstationary signal, a important measure of interests is how fast the value of the variables change. This is important to understand as it provides context for how long a signal must be sampled to before a meaningful representation of the signal can be calculated in a statistical sense. One way to quantify how fast the values of signal change is the autocorrelation function: 
\begin{equation}
R_{xx}(\tau) = \lim\limits_{T \rightarrow \infty} \frac{1}{T} \int_{0}^{T}x(t)x(t+\tau)dt
\end{equation}
The subscript $xx$ denotes that this is a measure of the response for the variable $xx$, $\tau$ is the time difference between the values at which the signal $x(t)$ is sampled. The auto collation for the two inputs considered above are expressed in figure~\ref{fig:Response_to_random_input_autocorrelation}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../figures/Response_to_random_input_autocorrelation.png}
	\caption{Responses from the autocorrelation function for the inputs shown in figure \ref{fig:Response_to_random_input_inputs} showing: (a) a sinusoidal; and (b) uniform random noise.}
	\label{fig:Response_to_random_input_autocorrelation}
\end{figure}
\noindent Note that the value of $\tau$ selected in the auto correlation function greatly affects its response for the sinusoidal input. This is because the values for the sinusoidal are highly correlated. To expand, the value at any time $t$ is greatly effected by the values immediately before and after it. This is not the case for the random input where the signal is not correlated and therefore there is little difference in changing the value of $\tau$ on the response of the autocorrelation function.  

Next, if we take the Fourier transform of the autocorrelation function we obtain the power spectral density (PSD) defined as:
\begin{equation}
S_{xx}(\omega) =\frac{1}{2 \pi} \int_{-\infty}^{\infty} R_{xx}(\tau) e^{-j \omega \tau}d \tau
\end{equation}
where the integral of $R_{xx}(\tau)$ changes the real number $\tau$ into the frequency-domain value $\omega$. The frequency spectrum is denoted with $S$ and the subscript of the considered variable $(\text{e.g., }S_{xx}(\omega))$.  The frequency spectrum for the two input cases considered are plotted in figure~\ref{fig:Response_to_random_input_PSD}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../figures/Response_to_random_input_PSD.png}
	\caption{Power spectral density plots for the inputs shown in figure \ref{fig:Response_to_random_input_inputs} showing: (a) a sinusoidal; and (b) uniform random noise.}
	\label{fig:Response_to_random_input_PSD}
\end{figure}
\noindent where the flat frequency response for the random input denotes that the random input is white noise input.  This flat frequency response in the frequency domain can be denoted $S_0$, such that $S_{ff}(\omega) = S_0$ or $S_{xx}(\omega) = S_0$, depending on whether the frequency spectrum of the input $(ff)$ or output  $(xx)$ is being considered. While a true white noise input would be perfectly flat, white noise is really just a theoretical concept as all real-world data will have some variation in the frequency domain as diagrammed in figure \ref{fig:Response_to_random_input_PSD}(b). 

Recall that $S_{xx}$ is the spectrum of the response of the system. For the one-DOF system considered here, we can express the arbitrary input as a series of impulse inputs as shown in section~\ref{sec:impulse_inputs}. This knowledge, along with the frequency response function can be used to relate the spectrum of the input $S_{ff}(\omega)$ to the output through the transfer function as:
\begin{equation}
S_{xx}(\omega) =  |H(j\omega)|^2\Bigg[\frac{1}{2 \pi } \int_{-\infty}^{\infty} R_{ff}(\tau) e^{-j \omega \tau}d  \tau  \Bigg] 
\end{equation}
This can also be expressed in symbolic form as:
\begin{equation}
S_{xx}(\omega) =  |H(j\omega)|^2 S_{ff}(\omega)
\end{equation}
where $R_{ff}$ denotes the autocorrelation function of $F(t)$ and $S_{ff}$ denotes the PSD of the forcing function $F(t)$. The notation $|H(j\omega)|^2$ is the square of the magnitude of the complex frequency response function. A more detailed derivation can be found in [Engineering Vibrations, Inman (2001)],[Random Vibrations, Spectral \& Wavelet Analysis, Newland (1993)], but here it is more important to study the results rather than the derivations. 


\begin{example}
	Consider the following system
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\textwidth]{../figures/1-DOF-spring_dashpot_mass_horizontal_forced.png}
		\caption{A spring-dashpot-mass model of a 1-DOF system with external excitation.}
	\end{figure}
	Calculate the PSD of the response $x(t)$ given that the PSD of the applied force $S_{ff}(\omega)$ is white noise. 
	
	\noindent\textbf{Solution:} From the system we know that the EOM is 
	\begin{equation}
	m\ddot{x}(t) +c\dot{x}(t) + kx(t) = F(t)
	\end{equation} 
	The frequency response function for this system is 
	\begin{eqnarray}
		H(j\omega) = \frac{1}{k-m\omega^2+c\omega j}
	\end{eqnarray}
	while the amplitude of the response is:
	\begin{eqnarray}
	H(\omega) = |H(j\omega)| = \frac{1}{\sqrt{(k-m\omega^2)^2+c^2\omega^2}}
	\end{eqnarray}
	Applying the equation that relates $S_{ff}(\omega)$ to $S_{xx}(\omega)$ we get:
	\begin{equation}
	S_{xx}(\omega) =  |H(j\omega)|^2 S_{ff}(\omega) = \bigg|\frac{1}{k-m\omega^2+c\omega j} \bigg|^2 S_{ff}(\omega) 
	\end{equation}
	White noise means the forcing function $S_{ff}(\omega)$ is constant across the frequency spectrum, therefore, $S_{ff}(\omega)=S_0$. Additionally as:
	\begin{equation}
	|H(j\omega)|^2 = \bigg|\frac{1}{k-m\omega^2+c\omega j} \bigg|^2 = \frac{1}{(k-m\omega^2)^2+c^2\omega^2}
	\end{equation}
	where the absolute value is the amplitude of the system. Therefore, we obtain:
	\begin{equation}
	S_{xx}(\omega) =  |H(j\omega)|^2 S_{0}= \frac{1}{(k-m\omega^2)^2+c^2\omega^2}S_0 = \frac{S_0}{(k-m\omega^2)^2+c^2\omega^2}
	\end{equation}
	Using various values for the elements in the system, the PSD for the system considered looks like:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{../figures/response_to_white_noise_with_annotation.png}
		\caption{Response for considered 1-DOF systems subjected to a white noise input.}
	\end{figure}
\end{example}  

Another useful quantity to consider is the expected output, in terms of its mean and variance, for a given input. Working within the constraint that the system will oscillate about zero, $E[x]=0$, the mean-square value can be directly related to the PSD function as:
\begin{equation}
E[x^2] = \overline{x^2} =   \int_{-\infty}^{\infty} |H(j\omega)|^2 S_{ff}(\omega) d\omega
\end{equation}
For a constant input $S_0$, as diagrammed in figure \ref{fig:Response_to_random_input_PSD}(b), the mean-square value can be expressed as:
\begin{equation}
E[x^2] = \overline{x^2} =   S_{0} \int_{-\infty}^{\infty} |H(j\omega)|^2 d\omega
\label{eq:variance_for_S_O}
\end{equation}

After inspecting the above equation, it becomes clear that to obtain the square of the expected value, a solution for  $\int_{-\infty}^{\infty} |H(j\omega)|^2 d\omega$ must be obtained. For cases where $S_{ff}(\omega) = S_0$ and as such $S_{ff}(\omega)$ can be pulled out of the integral, these integrals have been solved [Random Vibrations, Spectral \& Wavelet Analysis, Newland (1993)]. For example, given $\int_{-\infty}^{\infty} |H(j\omega)|^2 d\omega$:
\begin{equation}
\int_{-\infty}^{\infty} \bigg|\frac{B_0}{A_0+j \omega A_1} \bigg|^2 d\omega = \frac{\pi B_0^2}{A_0 A_1}
\end{equation} 
and
\begin{equation}
\int_{-\infty}^{\infty} \bigg|\frac{B_0 + j \omega B_1}{A_0+j \omega A_1 - \omega^2 A_2} \bigg|^2 d\omega = \frac{\pi (A_0 B_1^2 + A_2 B_0^2)}{A_0 A_1 A_2}
\end{equation} 
When combined with equation \ref{eq:variance_for_S_O}, these integrals allow for the easy calculation of the expected values. 

\begin{example}
	For system below, calculate the mean-square response of the system given that the the spectrum of the input force $F(t)$ is a perfect theoretical white noise.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\textwidth]{../figures/1-DOF-spring_dashpot_mass_horizontal_forced.png}
		\caption{A spring-dashpot-mass model of a 1-DOF system with external excitation.}
	\end{figure}
	\noindent\textbf{Solution:} Again, as the forcing function $S_{ff}(\omega)$ is constant across the frequency spectrum $S_{ff}(\omega)=S_0$ the mean-square response can be calculated as:
	\begin{equation}
		E[x^2] = \overline{x^2} =   S_{0} \int_{-\infty}^{\infty} |H(j\omega)|^2 d\omega
	\end{equation}
	Using the already tabulated response:
	\begin{equation}
		\int_{-\infty}^{\infty} \bigg|\frac{B_0 + j \omega B_1}{A_0+j \omega A_1 - \omega^2 A_2} \bigg|^2 d\omega = \frac{\pi (A_0 B_1^2 + A_2 B_0^2)}{A_0 A_1 A_2}
	\end{equation} 
	and the frequency response function for the system as derived in equation \ref{eq:frequency_response_function}:
	\begin{equation}
		H(j\omega) = \frac{1}{k-m\omega^2+c\omega j}
	\end{equation}
	when $B_0=1$, $B_1 = 0$, $A_0=k$, $A_1=c$, and $A_2 =m$. Therefore, using the tabulated expression we can show that:
	\begin{equation}
		E[x^2] = S_0 \frac{\pi m }{k c m} =  \frac{S_0 \pi}{k c}
	\end{equation} 
\end{example}			
			

\subsection{Inverse Laplace Transform} 

Consider that X(s) can be expanded in partial fractions as
\begin{equation}
X(s) = \frac{B(s)}{A(s)} = \frac{r_1}{s-p_1} + \frac{r_2}{s-p_2} + \cdots + \frac{r_n}{s-p_n} = \sum_{i=1}^{n}  \frac{r_i}{s-p_i}
\end{equation}
recall that $\Laplace{e^{p_0t}} =  \frac{1}{s-p_0}$ for $p_0$. The inverse Laplace Transform of $X(s)$ is a number:
\begin{equation}
x(t) = r_1 e^{p_1 t} + r_2 e^{p_2 t}  + \cdots + r_n e^{p_n t}  = \sum_{i=1}^{n}  r_i e^{p_i t}
\end{equation}
where the values $p_1, \;\; p_2, \;\; \cdots, \;\;  p_n$ are the roots of the demonstrator equation and are the poles of the system and may be complex. The values $r_1, \;\; r_2, \;\; \cdots, \;\;  r_n$ are called ``residuals'' and are defined as,
\begin{equation}
r_i = \lim\limits_{s \rightarrow p_i} \big[(s-p_i)X(s) \big]
\end{equation}
the MATLAB function \texttt{residue} will perform partial fraction expansion (PFD) and can be used to find the values. 

When complex poles appear in the system, they are conjugate pairs, i.e., 
\begin{equation}
p_{1,2} = \sigma \pm i \omega_d
\end{equation}
where $\sigma$ is the position of the poles on the real axis in the S-domain. This can also be written as $\sigma = \zeta \omega_n$. Partial fraction decomposition gives us:
\begin{equation}
X(s) = X_1(s) + X_2(s) = \frac{r_1}{s-p_1} + \frac{r_2}{s-p_2}
\label{eq:linear_X_s_with_residuales}
\end{equation}
where $A(s)$ and $B(s)$ are polynomial expansions, similar to equation~\ref{eq:transfer_function_polynominal_expansion}. The linearity of the system allows us to define $X(s) = X_1(s) + X_2(s)$ and therefore, $x(t) = x_1(t) + X_2(t)$ in the time domain. 

From equation~\ref{eq:linear_X_s_with_residuales}, $r_1$ and $r_1$ represent the residuals of the solution as a whole, but we want to extract the steady state (harmonic component) and transient state (exponential component) of the signals. For that, we need a joint residual value that we will call $r_0$. With this in mind, it can be shown that $r_1 = i r_0$, and that $r_2 = ir_0$
\begin{align}
x_1(t) &= \frac{-i r_0}{s-p_1}\text{, } p_1 = \sigma +i \omega_d  \\
&= \Laplace{X_1(s)}^{-1} \nonumber \\
&= \Laplace{\frac{-i r_0}{s-p_1}}^{-1} \nonumber \\
&= -i r_0 e^{p_1 t}  \nonumber \\
&= -i r_0 e^{(\sigma + i \omega_d) t}  \nonumber \\
&= -i r_0 e^{\sigma t}  e^{i \omega_d t}  \nonumber \\
&= -i r_0 e^{\sigma t}(\cos(\omega_d t) + i \sin(\omega_d t ))  \nonumber \\
&= r_0 e^{\sigma t}(-i \cos(\omega_d t) + \sin(\omega_d t )) \nonumber
\end{align}
similarly,
\begin{align}
x_2(t) &= \frac{i r_0}{i-p_1}\text{, } p_2 = \sigma +i \omega_d \\ 
&= r_0 e^{\sigma t}(i \cos(\omega_d t) + \sin(\omega_d t )) \nonumber 
\end{align}
the time series response can be rebuilt as 
\begin{align}
x_1(t) + x_2(t) &= r_0 e^{\sigma t}\big(-i \cos(\omega_d t) + \sin(\omega_d t ) \big) + r_0 e^{\sigma t}\big(i \cos(\omega_d t) + \sin(\omega_d t ) \big) \\ 
&= 2 r_0 e^{\sigma t} \sin (\omega_d t) \nonumber 
\end{align}
For stable systems, we we expect $\sigma > 0$ so, $\sigma = -|\sigma|$ as the poles are on the left hand side for a stable response
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{../figures/time_series_response_inverse_laplace.png}
		\caption{Time series response for the signal.}
	\end{figure}


\begin{example}

For the system in the S-domain expressed as,
\begin{equation}
X(s) = \frac{1}{s+2} \frac{2}{s+1}
\end{equation}
use MATLAB to compute the roots and residuals, and use the roots and residuals to compute $X(s)$.
\begin{align}
X(s) &= \frac{1}{s+2} \frac{2}{s+1} \\
& = \frac{(s+1)+2(s +2)}{(s+1)(s+2)} \nonumber \\
 &= \frac{3s+5}{s^2+3s+2}  \nonumber \\
&= \frac{B(s)}{A(s)} \nonumber 
\end{align}

\noindent The \texttt{residue} command can be used such as:

\lstset{linewidth=5.8in}
\begin{minipage}{1\textwidth}
  \begin{center}
    \lstset{%
caption=MATLAB code to find poles and resuduals.,
      basicstyle=\ttfamily\footnotesize\bfseries,
      frame=tb,
    }
\begin{lstlisting}
% Define B and A
B = [3, 5];
A = [1, 3, 2]

% find the poles and the residuales 
[r, p, k] = residue(B,A)

% use poles and residuals to calculate the A and B
[B_new, A_new] = residue(r, p, k)
\end{lstlisting}
  \end{center}
\end{minipage}
this returns $r_1 =1, \; \; r_2 =2, \; \; p_1 = -2, \; \; p_2 = -1, \; \; k=void$. Inserting this back into the expression yields 
\begin{equation}
\frac{r_1}{s-p_1} + \frac{r_2}{s-p_2} = \frac{1}{s+2} + \frac{2}{s+1}
\end{equation}
which checks out! Now, converting the partial fraction expansion back to the ratio of two polynomials results in $A(s) = 1,  \; \; 3,  \; \; 2$, and $B(s) = 3,  \; \; 5$. So this also check outs!
\end{example}

\subsection{Dominant Poles}

Given 
\begin{equation}
X(s) = \frac{B(s)}{A(s)} = \frac{r_1}{s-p_1} + \frac{r_2}{s-p_2} + \cdots + \frac{r_k}{s-p_k} + \cdots
\end{equation}
where $p_k$ is either a complex number ($p_k \in \mathbb{C}$) and a conjugate pair
\begin{equation}
p_k = \sigma_k \pm i\omega_k
\end{equation}
or a real number ($p_k \in \mathbb{R}$)
\begin{equation}
p_k = \sigma_k
\end{equation}

Again, knowing that the inverse Laplace of $X(s)$ results in
\begin{equation}
x(t) = r_1 e^{p_1t} + r_2 e^{p_2t} + \cdots + r_k e^{p_kt} + \cdots
\end{equation}
we can solve for the time series response. For controls, we are interested in the long term behavior of $x(t)$, i.e., to find $x_{ss} =  \lim\limits_{t\rightarrow \infty} [x(t)]$ which is the steady state response of the system. Note, that every term in the expansion will have the following form
\begin{equation}
r_k e^{p_kt} = r_ke^{\sigma_k t} \cdot e^{i \omega_k t}
\end{equation}
where: 
\begin{itemize}[noitemsep,topsep=0pt]
\item $r_ke^{\sigma_k t}$ is the exponential function
\item $e^{i \omega_k t}$ is the harmonic oscillation
\end{itemize}
we distinguish the following possible cases

\subsubsection{Case A (Unstable):}

If at least one $\sigma_k$ is positive, then $e^{\sigma_k t} \rightarrow \infty$, unstable system. 

\subsubsection{Case B  (Stable):} 

If system is stable, $\sigma_k<0$ (all poles on the left hand side) then:

\noindent \textbf{Case B1 - Dominant Poles on the imaginary axis:}  If terms with $\sigma_k=0$ exist, than these will be maintain oscillations as the rest of the poles die out. This means the poles placed on the imaginary axis, if they exist, are dominant poles. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/dominant_poles_B1.png}
	\caption{System with dominant poles on the imaginary axis.}
\end{figure}

\noindent \textbf{Case B2 - Dominant Poles to the left of the imaginary axis:} If terms with $\sigma_k=0$ do not exist, than the dominant poles are the poles closest to the imaginary axis because they take the longest to die out, having small $\sigma_k$ values (small damping). 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/dominant_poles_B2.png}
	\caption{System with dominant poles to the left of the imaginary axis.}
\end{figure}






































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{empty}
\newgeometry{top=0.5in, bottom=0.5in,left=0.5in, right=0.5in}
\vspace{-25ex}
\begin{center}
{\large{}\textbf{Table of Laplace Transforms for Vibrations}} \\
\normalsize{} This is a partial lists of important Laplace transforms for vibrations that assumes \\ zero initial conditions, $0 < t$, and $\zeta < 1$.
\end{center}

\vspace{0ex}
{\small
\renewcommand{\arraystretch}{1.5}
\begin{multicols}{2}
\begin{center}
\begin{tabbing}
\hspace*{1.5 in}\=\hspace{1.5in}\= \kill
	$f(t)$ \> $\Laplace{f(t)}=F(s)$ \> \\ \noindent\rule{8.0cm}{0.4pt} \\
	$\delta(t)$	\> 1 \> \LTNUM \\ \\
	$\delta(t-t_0)$ \> $e^{-st_0}$ \>\LTNUM \\ \\
	$1$       			 \> $\dfrac{1}{s}$           \>\LTNUM \\ \\
	$e^{at}$ 	\> $\dfrac{1}{s-a}$ 	\>\LTNUM \\ \\
	$\sin (\omega t) $ 	\> $\dfrac{\omega}{s^2+\omega^2}$ \>\LTNUM \\ \\
	$\cos (\omega t) $ 	\> $\dfrac{s}{s^2+\omega^2}$ \>\LTNUM \\ \\
	
	$\sinh (\omega t) $	\> $\dfrac{\omega}{s^2-\omega^2}$ \>\LTNUM \\ \\
	$\cosh (\omega t) $	\> $\dfrac{s}{s^2-\omega^2}$ \>\LTNUM \\ \\
	$\dfrac{1}{\omega^2}\big(1-\cos(\omega t)\big)$ \> $\dfrac{1}{s(s^2+\omega^2)}$ \>\LTNUM \\ \\
	$\dfrac{1}{\omega_d}e^{-\zeta \omega t}\sin(\omega_d t)$ \> $\dfrac{1}{s^2+2\zeta \omega s+\omega^2}$ \>\LTNUM \\ \\
	$1-\dfrac{\omega}{\omega_d}e^{-\zeta \omega t}\sin(\omega_d t + \phi)$, $\phi =\cos^{-1}(\zeta) \dots $ \\ \> $\dfrac{\omega^2}{s(s^2+2\zeta \omega s+\omega^2)}$ \>\LTNUM \\ \\
	$\dfrac{t^{n-1}}{(n-1)!}$, $ n=1,2,\dots $ \> $\dfrac{1}{s^n}$ \>\LTNUM \\ \\
	$t^n $, $n=1,2,\dots$     \> $\dfrac{n!}{s^{n+1}}$    \>\LTNUM \\ \\
	$t^ne^{\omega t}$, $ n=1,2,\dots $	\> $\dfrac{n!}{(s-\omega)^{n+1}}$	\>\LTNUM \\ \\
	$\dfrac{1}{\omega}(1-e^{-\omega t}) $	\> $\dfrac{1}{s(s+\omega)}$	\>\LTNUM \\ \\
	$\dfrac{1}{\omega^2}(e^{-\omega t}+\omega t - 1) $	\> $\dfrac{1}{s^2(s+\omega)}$	\>\LTNUM \\ \\
\end{tabbing}
\end{center}

\columnbreak

\begin{center}
\begin{tabbing}
\hspace*{1.5in}\=\hspace{1.5in}\= \kill
	$f(t)$ \> $\Laplace{f(t)}=F(s)$ \> \\ \noindent\rule{8.0cm}{0.4pt} \\
	$\dfrac{1}{\omega^3}\big(\omega t - \sin(\omega t)\big) $	\> $\dfrac{1}{s^2(s^2+\omega^2)}$	\>\LTNUM \\ \\
	$\dfrac{1}{2\omega^3}\big(\sin(\omega t) - \omega t \cos(\omega t)\big) \dots $	\\ \> $\dfrac{1}{(s^2+\omega^2)^2}$	\>\LTNUM \\ \\
	$\dfrac{t}{2\omega} \sin(\omega t)$	 \> $\dfrac{s}{(s^2+\omega^2)^2}$	\>\LTNUM \\ \\
	$t\sin (\omega t) $  	\> $\dfrac{2\omega s}{(s^2+\omega^2)^2}$ \>\LTNUM \\ \\
	$t\cos (\omega t) $  	\> $\dfrac{s^2-\omega^2}{(s^2+\omega^2)^2}$ \>\LTNUM \\ \\
	$e^{at}\sin (\omega t) $	\> $\dfrac{\omega}{(s-a)^2+\omega^2}$  \>\LTNUM \\ \\
	$e^{at}\cos (\omega t) $	\> $\dfrac{s-a}{(s-a)^2+\omega^2}$  \>\LTNUM \\ \\
	$e^{at}\sinh (\omega t) $	\> $\dfrac{\omega}{(s-a)^2-\omega^2}$  \>\LTNUM \\ \\
	$e^{at}\cosh (\omega t) $	\> $\dfrac{s-a}{(s-a)^2-\omega^2}$  \>\LTNUM \\ \\
	$\dfrac{1}{\omega_2}\sin(\omega_2 t) - \dfrac{1}{\omega_1}\sin (\omega_1 t) \dots $	 \\ \> $\dfrac{\omega_1^2-\omega_2^2}{(s^2+\omega_1^2)(s^2+\omega_2^2)}$	\>\LTNUM \\ \\
	$\cos(\omega_2 t) - \cos (\omega_1 t)  $	\> $\dfrac{s(\omega_1^2-\omega_2^2)}{(s^2+\omega_1^2)(s^2+\omega_2^2)}$	\>\LTNUM \\ \\
	$e^{at}f(t)$	\> $F(s-a)$	\>\LTNUM \\ \\ 
	$f(t-a)\Phi(t-a)$ \> $e^{-as}F(s)$ \>\LTNUM \\ \\
	$\Phi(t-a)$ \> $\dfrac{e^{-as}}{s}$ \>\LTNUM \\ \\
	$f'(t)$ 	\> $sF(s) - f(0)$ \>\LTNUM \\ \\
\end{tabbing}
\end{center}
\end{multicols}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%
%	\pagebreak
%	\renewcommand{\thepage}{}
%	\renewcommand\refname{References Cited}
%	\pagestyle{plain}
%	\bibliographystyle{Downey_NSF}
%	\bibliography{Chapter_1_Basic_Concepts}


















\end{document}

